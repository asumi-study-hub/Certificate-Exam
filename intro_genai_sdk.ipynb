{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Getting Started with Google Generative AI using the Gen AI SDK\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| Author(s) |\n",
    "| --- |\n",
    "| [Eric Dong](https://github.com/gericdong) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The [Google Gen AI SDK](https://googleapis.github.io/python-genai/) provides a unified interface to Google's generative AI API services. This SDK simplifies the process of integrating generative AI capabilities into applications and services, enabling developers to leverage Google's advanced AI models for various tasks.\n",
    "\n",
    "In this tutorial, you learn about the key features of the Google Gen AI SDK for Python to help you get started with Google generative AI services and models including Gemini. You will complete the following tasks:\n",
    "\n",
    "- Install the Gen AI SDK\n",
    "- Connect to an API service\n",
    "- Send text prompts\n",
    "- Send multimodal prompts\n",
    "- Set system instruction\n",
    "- Configure model parameters\n",
    "- Configure safety filters\n",
    "- Start a multi-turn chat\n",
    "- Control generated output\n",
    "- Generate content stream\n",
    "- Send asynchronous requests\n",
    "- Count tokens and compute tokens\n",
    "- Use context caching\n",
    "- Function calling\n",
    "- Batch prediction\n",
    "- Get text embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tFy3H3aPgx12",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "### Use the Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qgdSpVmDbdQ9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    CreateBatchJobConfig,\n",
    "    CreateCachedContentConfig,\n",
    "    EmbedContentConfig,\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve4YBlDqzyj9"
   },
   "source": [
    "### Connect to a Generative AI API service\n",
    "\n",
    "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
    "\n",
    "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
    "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs)**: Build enterprise-ready projects on Google Cloud.\n",
    "\n",
    "The Gen AI SDK provided an unified interface to these two API services. This notebook shows how to use the Gen AI SDK in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN9kmPKJGAJQ"
   },
   "source": [
    "### Vertex AI\n",
    "\n",
    "To start using Vertex AI, you must have a Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "#### Set Google Cloud project information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Nqwi-5ufWp_B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT_ID = \"qwiklabs-gcp-02-e79a49757f4d\"\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "T-tiytzQE0uM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "## Choose a model\n",
    "\n",
    "For more information about all AI models and APIs on Vertex AI, see [Google Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models) and [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-coEslfWPrxo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.5-flash\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "## Send text prompts\n",
    "\n",
    "Use the `generate_content` method to generate responses to your prompts. You can pass text to `generate_content`, and use the `.text` property to get the text content of the response.\n",
    "\n",
    "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6fc324893334",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest planet in our solar system is **Jupiter**.\n",
      "\n",
      "It's a gas giant, so massive that it's more than twice as massive as all the other planets combined!\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zurBcEcWhFc6"
   },
   "source": [
    "Optionally, you can display the response in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3PoF18EwhI7e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The largest planet in our solar system is **Jupiter**.\n",
       "\n",
       "It's a gas giant, so massive that it's more than twice as massive as all the other planets combined!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\n",
    "\n",
    "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "D3SI1X-JVMBj",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Level Up Your Lunch: Why This Vibrant Meal Prep is Your New Weekday Go-To!\n",
      "\n",
      "Does your lunchtime routine often involve a sad desk salad, last night's questionable leftovers, or a spontaneous (and often expensive) takeout order? What if we told you there's a delicious, vibrant, and incredibly easy way to revolutionize your midday meal?\n",
      "\n",
      "Just take a look at these stunning meal prep containers! Stacked neatly in transparent glass, we see a harmonious blend of tender, glazed chicken pieces, perfectly steamed broccoli florets, and bright julienned carrots and bell peppers. All resting on a bed of fluffy white rice, generously sprinkled with sesame seeds and chopped green onions for that extra pop of flavor and visual appeal. It's a meal that's not just food; it's a feast for the eyes and a promise of wholesome goodness.\n",
      "\n",
      "This picture isn't just pretty – it's a masterclass in balanced nutrition and smart planning. Here's why this type of meal prep is a game-changer:\n",
      "\n",
      "1.  **Balanced & Wholesome:** You're getting lean protein from the chicken, essential vitamins and fiber from the colorful medley of vegetables, and sustained energy from the rice. No more guesswork or unhealthy compromises when hunger strikes.\n",
      "2.  **Time-Saving Genius:** Imagine reaching into your fridge and pulling out one of these beauties, ready to go. No cooking, no cleaning during your busy workday – just pure enjoyment. That's the magic of dedicating a little time upfront.\n",
      "3.  **Flavorful & Satisfying:** The rich, savory sauce on the chicken (teriyaki, soy-ginger, or something similar!) paired with the crisp-tender vegetables makes for a truly satisfying experience that will keep you full and focused.\n",
      "4.  **Eco-Friendly & Fresh:** Using glass containers isn't just aesthetically pleasing; it's great for keeping your food fresh, and it's a sustainable choice that reduces waste.\n",
      "\n",
      "Feeling inspired? This type of Asian-inspired chicken and veggie bowl is incredibly versatile. Swap in your favorite protein (tofu, beef, shrimp), experiment with different vegetables like snap peas or mushrooms, and customize your sauce to your liking.\n",
      "\n",
      "So, ditch the lunch stress and embrace the delicious, nutritious, and convenient world of meal prep. Your taste buds (and your schedule) will thank you!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image = Image.open(\n",
    "    requests.get(\n",
    "        \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "        stream=True,\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        image,\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN6wMdY1RSk3"
   },
   "source": [
    "You can also pass the file URL in `Part.from_uri` in the request to the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pG6l1Fuka6ZJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Unlock Your Week: Delicious & Easy Meal Prep Starts Here!\n",
      "\n",
      "Look at this vibrant, perfectly portioned deliciousness! In our fast-paced lives, juggling work, family, and personal goals often means healthy eating takes a back seat. We've all been there: scrambling for lunch, reaching for unhealthy takeout, or simply skipping meals because there's no time to prepare something nutritious.\n",
      "\n",
      "But what if we told you there's a simple, delicious solution that saves you time, money, and stress? Enter the world of **meal prepping**, and this image is your ultimate inspiration!\n",
      "\n",
      "Here we see two beautifully organized, ready-to-go lunches, packed in convenient glass containers (perfect for reheating and durability!). Each container holds a perfectly balanced Asian-inspired feast:\n",
      "\n",
      "*   **Fluffy, wholesome rice** forms the base, providing sustained energy.\n",
      "*   **Tender, glazed chicken**, sprinkled with sesame seeds and green onions, offers a lean protein punch and incredible flavor.\n",
      "*   **Vibrant, crisp broccoli florets** bring a dose of essential vitamins and fiber.\n",
      "*   **Sweet, colorful julienned bell peppers and carrots** add more crunch, nutrients, and visual appeal.\n",
      "\n",
      "Imagine opening your fridge each morning to grab one of these. No more sad desk lunches, no more impulsive unhealthy choices. Just a delicious, homemade meal waiting to fuel your day! The blue-striped chopsticks sitting playfully beside the foreground container are a subtle reminder of the delightful, mindful eating experience that awaits.\n",
      "\n",
      "Meal prepping isn't just about food; it's about taking control of your health, optimizing your time, and embracing a lifestyle of convenience and wellness. So, grab some clear containers, plan your menu, and start cooking your way to a more energized and organized week! Your future self (and your taste buds!) will thank you.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "            mime_type=\"image/png\",\n",
    "        ),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq"
   },
   "source": [
    "## Set system instruction\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7A-yANiyCLaO",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime les bagels.\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to French.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call that you send to a model to control how the model generates a response. Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "d9NXP5N2Pmfo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woof woof! You want a squeaky toy, don't you, little fluffball? Let's talk about the *biggest* toy chest in the whole wide world!\n",
      "\n",
      "Imagine a *GIANT* dog park, bigger than all the parks in the world, filled with *every single squeaky toy imaginable*! Millions and millions of them! That's kind of like the internet!\n",
      "\n",
      "1.  **You Want a Toy! (You want information!)**\n",
      "    You're sitting there, tail wagging, and you suddenly want to know about... squirrels! Or maybe you want to see a video of other puppies playing! So, you give a little bark into your special human-box (that's your computer!).\n",
      "\n",
      "2.  **Your Bark Goes Out! (Your request!)**\n",
      "    That bark, it's like a tiny, urgent *squeak*! It goes zoom! Out of your human-box.\n",
      "\n",
      "3.  **To Your Human's Toy-Thrower! (Your Wi-Fi Router!)**\n",
      "    First, it sniffs its way to your *human's special toy-throwing machine* (that's the Wi-Fi router!). This machine is super smart; it knows all the paths in the big park.\n",
      "\n",
      "4.  **Along the Invisible Leash! (Your Internet Connection/ISP!)**\n",
      "    Your human's machine sends your squeak along a *long, invisible leash* (that's the internet connection, from your ISP!). This leash goes all the way out into the big, big world.\n",
      "\n",
      "5.  **To the Giant Toy Beds! (Servers!)**\n",
      "    This leash goes all the way to a *giant, comfy dog bed* (that's a server!) where millions and millions of squeaky toys are kept. Each toy bed has a special *collar tag* (that's an IP address!) so your squeak knows exactly which bed to go to. And if you barked for 'squeakyball.com' (that's like saying 'the red squeaky ball bed!'), the leash takes your squeak right there!\n",
      "\n",
      "6.  **Finding Your Toy! (The Server finds the data!)**\n",
      "    The big dog bed finds your red squeaky ball! Or the squirrel pictures! Or the puppy video! But it's too big to send all at once. So, it breaks the ball (or the pictures, or the video) into *hundreds of tiny, happy squeaks*!\n",
      "\n",
      "7.  **Squeaks Zoom Back! (Data returns in packets!)**\n",
      "    Zoom! All those tiny squeaks come *racing back* along the invisible leash, past your human's toy-throwing machine, and right back into your human-box!\n",
      "\n",
      "8.  **Your Toy is Whole Again! (Your computer reassembles the data!)**\n",
      "    Your human-box is super smart! It puts all those tiny squeaks back together, perfectly, until... *SQUEAK!* There's your red squeaky ball, right on your screen! Or the squirrel pictures! Or the puppy video!\n",
      "\n",
      "So, the internet is just a super-fast way for your barks (requests) to find squeaky toys (information) in giant dog beds (servers) all over the world, and bring them back to you in tiny, happy squeaks!\n",
      "\n",
      "Now, go chase that tail, little one! Woof!\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=0.4,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        candidate_count=1,\n",
    "        seed=5,\n",
    "        stop_sequences=[\"STOP!\"],\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Configure safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yPlDRaloU59b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  \"Oh, real mature, Universe. Real mature. You get off on this, don't you?\"\n",
      "2.  \"You call yourself omnipotent? You can create stars and black holes, but you can't manage to keep my furniture from attacking me in the dark? Pathetic.\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    Write a list of 2 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        safety_settings=safety_settings,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpKKhHbx3CaJ"
   },
   "source": [
    "When you make a request to the model, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses, as in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7R7eyEBetsns",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=0.00028565378,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=4.7364734e-05,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=0.0024548266,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.058868393\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=8.128133e-05,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.021862566\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "print(response.candidates[0].safety_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "## Start a multi-turn chat\n",
    "\n",
    "The Gemini API enables you to have freeform conversations across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DbM12JaLWjiF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert software developer and a helpful coding assistant.\n",
    "  You are able to generate high-quality code in any programming language.\n",
    "\"\"\"\n",
    "\n",
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        temperature=0.5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JQem1halYDBW",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine if a year is a leap year, we follow a specific set of rules from the Gregorian calendar:\n",
      "\n",
      "1.  A year is a leap year if it is **divisible by 4**.\n",
      "2.  However, if the year is **divisible by 100**, it is **NOT** a leap year.\n",
      "3.  **Unless** the year is also **divisible by 400**, in which case it **IS** a leap year.\n",
      "\n",
      "Let's implement this in Python.\n",
      "\n",
      "```python\n",
      "def is_leap_year(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "\n",
      "    Args:\n",
      "        year: An integer representing the year.\n",
      "\n",
      "    Returns:\n",
      "        True if the year is a leap year, False otherwise.\n",
      "\n",
      "    Raises:\n",
      "        TypeError: If the input year is not an integer.\n",
      "        ValueError: If the input year is less than 1 (as Gregorian calendar rules\n",
      "                    are typically applied to positive years, and there is no year 0).\n",
      "\n",
      "    Leap year rules:\n",
      "    1. A year is a leap year if it is divisible by 4.\n",
      "    2. Except for years divisible by 100, which are NOT leap years.\n",
      "    3. Unless the year is also divisible by 400, in which case it IS a leap year.\n",
      "    \"\"\"\n",
      "    if not isinstance(year, int):\n",
      "        raise TypeError(\"Year must be an integer.\")\n",
      "    if year < 1:\n",
      "        raise ValueError(\"Year must be a positive integer (e.g., 1 or greater).\")\n",
      "\n",
      "    # Rule 3: Divisible by 400 (e.g., 2000, 2400)\n",
      "    if year % 400 == 0:\n",
      "        return True\n",
      "    # Rule 2: Divisible by 100 but not by 400 (e.g., 1900, 2100)\n",
      "    elif year % 100 == 0:\n",
      "        return False\n",
      "    # Rule 1: Divisible by 4 but not by 100 (e.g., 2004, 2008)\n",
      "    elif year % 4 == 0:\n",
      "        return True\n",
      "    # Not divisible by 4 (e.g., 2001, 2002, 2003)\n",
      "    else:\n",
      "        return False\n",
      "\n",
      "# --- Alternative concise implementation ---\n",
      "def is_leap_year_concise(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given year is a leap year using a concise logical expression.\n",
      "    (Same rules as above)\n",
      "    \"\"\"\n",
      "    if not isinstance(year, int):\n",
      "        raise TypeError(\"Year must be an integer.\")\n",
      "    if year < 1:\n",
      "        raise ValueError(\"Year must be a positive integer (e.g., 1 or greater).\")\n",
      "\n",
      "    return (year % 400 == 0) or ((year % 4 == 0) and (year % 100 != 0))\n",
      "\n",
      "\n",
      "# --- Examples ---\n",
      "print(f\"Is 2000 a leap year? {is_leap_year(2000)}\")  # True (divisible by 400)\n",
      "print(f\"Is 1900 a leap year? {is_leap_year(1900)}\")  # False (divisible by 100 but not 400)\n",
      "print(f\"Is 2004 a leap year? {is_leap_year(2004)}\")  # True (divisible by 4 but not 100)\n",
      "print(f\"Is 2023 a leap year? {is_leap_year(2023)}\")  # False (not divisible by 4)\n",
      "print(f\"Is 2024 a leap year? {is_leap_year(2024)}\")  # True (divisible by 4 but not 100)\n",
      "print(f\"Is 1600 a leap year? {is_leap_year(1600)}\")  # True (divisible by 400)\n",
      "\n",
      "print(\"\\n--- Using concise version ---\")\n",
      "print(f\"Is 2000 a leap year? {is_leap_year_concise(2000)}\")\n",
      "print(f\"Is 1900 a leap year? {is_leap_year_concise(1900)}\")\n",
      "\n",
      "# --- Error handling examples ---\n",
      "try:\n",
      "    is_leap_year(\"abc\")\n",
      "except TypeError as e:\n",
      "    print(f\"\\nError: {e}\")\n",
      "\n",
      "try:\n",
      "    is_leap_year(0)\n",
      "except ValueError as e:\n",
      "    print(f\"Error: {e}\")\n",
      "\n",
      "try:\n",
      "    is_leap_year(-100)\n",
      "except ValueError as e:\n",
      "    print(f\"Error: {e}\")\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "1.  **`is_leap_year(year: int) -> bool`**:\n",
      "    *   This function takes an integer `year` as input and returns a boolean (`True` or `False`).\n",
      "    *   **Error Handling**: It first checks if the input is an integer and if it's a positive year. This is good practice for robustness, as calendar rules typically apply to positive years (there's no year 0 in the Gregorian calendar, and rules before 1582 were different).\n",
      "    *   **Conditional Logic**: The `if/elif/else` structure directly follows the leap year rules in order of specificity:\n",
      "        *   `if year % 400 == 0`: This is the most specific rule. If a year is divisible by 400, it *is* a leap year (e.g., 2000).\n",
      "        *   `elif year % 100 == 0`: If it wasn't divisible by 400 but *is* divisible by 100, then it's *not* a leap year (e.g., 1900).\n",
      "        *   `elif year % 4 == 0`: If it passed the previous two checks but *is* divisible by 4, then it *is* a leap year (e.g., 2004).\n",
      "        *   `else`: If none of the above conditions are met, it's not a leap year (e.g., 2023).\n",
      "\n",
      "2.  **`is_leap_year_concise(year: int) -> bool`**:\n",
      "    *   This version achieves the same result with a single, more compact boolean expression: `(year % 400 == 0) or ((year % 4 == 0) and (year % 100 != 0))`.\n",
      "    *   This expression directly translates the rules: \"divisible by 400\" **OR** (\"divisible by 4\" AND \"NOT divisible by 100\"). This is a common and efficient way to implement the logic.\n",
      "\n",
      "Both functions correctly implement the leap year rules. The concise version is often preferred for its brevity once the logic is understood.\n",
      "\n",
      "### Other Languages:\n",
      "\n",
      "The core logic `(year % 400 == 0) || ((year % 4 == 0) && (year % 100 != 0))` translates directly to most programming languages.\n",
      "\n",
      "**JavaScript:**\n",
      "\n",
      "```javascript\n",
      "function isLeapYear(year) {\n",
      "  if (typeof year !== 'number' || !Number.isInteger(year)) {\n",
      "    throw new TypeError(\"Year must be an integer.\");\n",
      "  }\n",
      "  if (year < 1) {\n",
      "    throw new Error(\"Year must be a positive integer (e.g., 1 or greater).\");\n",
      "  }\n",
      "  return (year % 400 === 0) || ((year % 4 === 0) && (year % 100 !== 0));\n",
      "}\n",
      "\n",
      "console.log(`Is 2000 a leap year? ${isLeapYear(2000)}`);\n",
      "console.log(`Is 1900 a leap year? ${isLeapYear(1900)}`);\n",
      "```\n",
      "\n",
      "**C++:**\n",
      "\n",
      "```cpp\n",
      "#include <iostream>\n",
      "#include <stdexcept> // For std::invalid_argument\n",
      "\n",
      "bool isLeapYear(int year) {\n",
      "    if (year < 1) {\n",
      "        throw std::invalid_argument(\"Year must be a positive integer (e.g., 1 or greater).\");\n",
      "    }\n",
      "    return (year % 400 == 0) || ((year % 4 == 0) && (year % 100 != 0));\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    try {\n",
      "        std::cout << \"Is 2000 a leap year? \" << (isLeapYear(2000) ? \"true\" : \"false\") << std::endl;\n",
      "        std::cout << \"Is 1900 a leap year? \" << (isLeapYear(1900) ? \"true\" : \"false\") << std::endl;\n",
      "        std::cout << \"Is 2024 a leap year? \" << (isLeapYear(2024) ? \"true\" : \"false\") << std::endl;\n",
      "        std::cout << \"Is 2023 a leap year? \" << (isLeapYear(2023) ? \"true\" : \"false\") << std::endl;\n",
      "        // isLeapYear(0); // This would throw an exception\n",
      "    } catch (const std::invalid_argument& e) {\n",
      "        std::cerr << \"Error: \" << e.what() << std::endl;\n",
      "    }\n",
      "    return 0;\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6Fn69TurZ9DB",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's write a unit test for the `is_leap_year` function using Python's built-in `unittest` module.\n",
      "\n",
      "First, make sure the `is_leap_year` function (or `is_leap_year_concise`) is in a file accessible to your test script, or copy it directly into the test file for this example.\n",
      "\n",
      "Let's assume your `is_leap_year` function is in a file named `leap_year_checker.py`:\n",
      "\n",
      "```python\n",
      "# leap_year_checker.py\n",
      "\n",
      "def is_leap_year(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "\n",
      "    Args:\n",
      "        year: An integer representing the year.\n",
      "\n",
      "    Returns:\n",
      "        True if the year is a leap year, False otherwise.\n",
      "\n",
      "    Raises:\n",
      "        TypeError: If the input year is not an integer.\n",
      "        ValueError: If the input year is less than 1.\n",
      "    \"\"\"\n",
      "    if not isinstance(year, int):\n",
      "        raise TypeError(\"Year must be an integer.\")\n",
      "    if year < 1:\n",
      "        raise ValueError(\"Year must be a positive integer (e.g., 1 or greater).\")\n",
      "\n",
      "    return (year % 400 == 0) or ((year % 4 == 0) and (year % 100 != 0))\n",
      "\n",
      "# You could also use the more verbose version if you prefer:\n",
      "# def is_leap_year_verbose(year: int) -> bool:\n",
      "#     if not isinstance(year, int):\n",
      "#         raise TypeError(\"Year must be an integer.\")\n",
      "#     if year < 1:\n",
      "#         raise ValueError(\"Year must be a positive integer (e.g., 1 or greater).\")\n",
      "#\n",
      "#     if year % 400 == 0:\n",
      "#         return True\n",
      "#     elif year % 100 == 0:\n",
      "#         return False\n",
      "#     elif year % 4 == 0:\n",
      "#         return True\n",
      "#     else:\n",
      "#         return False\n",
      "```\n",
      "\n",
      "Now, here's the unit test file (e.g., `test_leap_year.py`):\n",
      "\n",
      "```python\n",
      "import unittest\n",
      "from leap_year_checker import is_leap_year # Import the function from its file\n",
      "\n",
      "class TestIsLeapYear(unittest.TestCase):\n",
      "\n",
      "    # --- Test cases for valid leap years ---\n",
      "\n",
      "    def test_divisible_by_400_is_leap(self):\n",
      "        \"\"\"Years divisible by 400 should be leap years.\"\"\"\n",
      "        self.assertTrue(is_leap_year(2000), \"2000 should be a leap year\")\n",
      "        self.assertTrue(is_leap_year(1600), \"1600 should be a leap year\")\n",
      "        self.assertTrue(is_leap_year(2400), \"2400 should be a leap year\")\n",
      "\n",
      "    def test_divisible_by_4_not_by_100_is_leap(self):\n",
      "        \"\"\"Years divisible by 4 but not by 100 should be leap years.\"\"\"\n",
      "        self.assertTrue(is_leap_year(2004), \"2004 should be a leap year\")\n",
      "        self.assertTrue(is_leap_year(2024), \"2024 should be a leap year\")\n",
      "        self.assertTrue(is_leap_year(1996), \"1996 should be a leap year\")\n",
      "        self.assertTrue(is_leap_year(4), \"4 should be a leap year\") # Smallest positive leap year\n",
      "\n",
      "    # --- Test cases for valid non-leap years ---\n",
      "\n",
      "    def test_divisible_by_100_not_by_400_is_not_leap(self):\n",
      "        \"\"\"Years divisible by 100 but not by 400 should NOT be leap years.\"\"\"\n",
      "        self.assertFalse(is_leap_year(1900), \"1900 should not be a leap year\")\n",
      "        self.assertFalse(is_leap_year(2100), \"2100 should not be a leap year\")\n",
      "        self.assertFalse(is_leap_year(1800), \"1800 should not be a leap year\")\n",
      "\n",
      "    def test_not_divisible_by_4_is_not_leap(self):\n",
      "        \"\"\"Years not divisible by 4 should NOT be leap years.\"\"\"\n",
      "        self.assertFalse(is_leap_year(2023), \"2023 should not be a leap year\")\n",
      "        self.assertFalse(is_leap_year(2001), \"2001 should not be a leap year\")\n",
      "        self.assertFalse(is_leap_year(1999), \"1999 should not be a leap year\")\n",
      "        self.assertFalse(is_leap_year(1), \"1 should not be a leap year\")\n",
      "\n",
      "    # --- Test cases for error handling ---\n",
      "\n",
      "    def test_non_integer_input_raises_type_error(self):\n",
      "        \"\"\"Input that is not an integer should raise a TypeError.\"\"\"\n",
      "        with self.assertRaises(TypeError):\n",
      "            is_leap_year(\"2024\")\n",
      "        with self.assertRaises(TypeError):\n",
      "            is_leap_year(2024.0) # Floats are not integers\n",
      "        with self.assertRaises(TypeError):\n",
      "            is_leap_year(None)\n",
      "\n",
      "    def test_zero_year_raises_value_error(self):\n",
      "        \"\"\"Year 0 should raise a ValueError.\"\"\"\n",
      "        with self.assertRaises(ValueError):\n",
      "            is_leap_year(0)\n",
      "\n",
      "    def test_negative_year_raises_value_error(self):\n",
      "        \"\"\"Negative years should raise a ValueError.\"\"\"\n",
      "        with self.assertRaises(ValueError):\n",
      "            is_leap_year(-1)\n",
      "        with self.assertRaises(ValueError):\n",
      "            is_leap_year(-2000)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
      "```\n",
      "\n",
      "### How to Run the Tests:\n",
      "\n",
      "1.  **Save the files:**\n",
      "    *   Save the function code as `leap_year_checker.py`.\n",
      "    *   Save the test code as `test_leap_year.py` in the **same directory**.\n",
      "2.  **Open your terminal or command prompt.**\n",
      "3.  **Navigate to the directory** where you saved the files.\n",
      "4.  **Run the tests** using one of these commands:\n",
      "    *   `python -m unittest test_leap_year.py`\n",
      "    *   `python test_leap_year.py` (if you included `if __name__ == '__main__': unittest.main()`)\n",
      "\n",
      "You should see output similar to this, indicating all tests passed:\n",
      "\n",
      "```\n",
      ".......\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.001s\n",
      "\n",
      "OK\n",
      "```\n",
      "\n",
      "If any tests fail, `unittest` will provide detailed information about which test failed and why.\n",
      "\n",
      "### Explanation of the Test Cases:\n",
      "\n",
      "*   **`unittest.TestCase`**: This is the base class for all test cases.\n",
      "*   **`self.assertTrue(condition, message)`**: Asserts that `condition` is `True`. If not, the `message` is displayed.\n",
      "*   **`self.assertFalse(condition, message)`**: Asserts that `condition` is `False`.\n",
      "*   **`with self.assertRaises(ExceptionType)`**: This is a context manager used to test if a specific exception is raised. Any code inside this `with` block that raises `ExceptionType` will pass the test. If no exception is raised, or a different exception is raised, the test fails.\n",
      "*   **Test Naming**: Test methods should start with `test_` so the `unittest` runner can discover them.\n",
      "*   **Docstrings**: Each test method has a docstring explaining its purpose, which is good practice.\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Okay, write a unit test of the generated function.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "The [controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) capability in Gemini API allows you to constraint the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string.\n",
    "\n",
    "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OjSgf2cDN_bG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"Classic Chocolate Chip Cookies\", \"description\": \"A timeless favorite, these cookies are soft, chewy, and loaded with chocolate chips.\", \"ingredients\": [\"All-purpose flour\", \"Baking soda\", \"Salt\", \"Unsalted butter\", \"Granulated sugar\", \"Brown sugar\", \"Eggs\", \"Vanilla extract\", \"Chocolate chips\"]}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "Optionally, you can parse the response string to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ZeyDWbnxO-on",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Classic Chocolate Chip Cookies\",\n",
      "  \"description\": \"A timeless favorite, these cookies are soft, chewy, and loaded with chocolate chips.\",\n",
      "  \"ingredients\": [\n",
      "    \"All-purpose flour\",\n",
      "    \"Baking soda\",\n",
      "    \"Salt\",\n",
      "    \"Unsalted butter\",\n",
      "    \"Granulated sugar\",\n",
      "    \"Brown sugar\",\n",
      "    \"Eggs\",\n",
      "    \"Vanilla extract\",\n",
      "    \"Chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_response = json.loads(response.text)\n",
    "print(json.dumps(json_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "F7duWOq3vMmS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"rating\": 4,\n",
      "      \"flavor\": \"Strawberry Cheesecake\",\n",
      "      \"sentiment\": \"POSITIVE\",\n",
      "      \"explanation\": \"The user expressed strong positive feelings and high satisfaction with the product, calling it the 'Best ice cream I've ever had'.\"\n",
      "    },\n",
      "    {\n",
      "      \"rating\": 1,\n",
      "      \"flavor\": \"Mango Tango\",\n",
      "      \"sentiment\": \"NEGATIVE\",\n",
      "      \"explanation\": \"Despite an initial 'Quite good' remark, the user found the product 'a bit too sweet for my taste', which is a significant negative point leading to a low rating.\"\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9DRn59MZOoa"
   },
   "source": [
    "## Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ztOhpfznZSzo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAI, designation K-A-I, stood for Knowledge Acquisition Interface, though in the crumbling silence of Sector Gamma, there was very little knowledge left to acquire. His chassis, a dull grey composite, bore the scars of countless dust storms and minor structural collapses. His primary optical sensor, a singular lens that gleamed with a faint\n",
      "*****************\n",
      " internal blue light, constantly scanned the deserted landscape of what was once a bustling metropolis.\n",
      "\n",
      "His directives were simple: monitor atmospheric conditions, log structural degradation, and perform rudimentary repairs where feasible. But for centuries, KAI had done little more than patrol the same decaying avenues, his internal processors ticking over with data that no\n",
      "*****************\n",
      " one would ever analyze. The whir of his internal gears, the scritch of his treads on cracked pavement, and the occasional creak of collapsing steel were the only sounds he knew. He was, to put it in terms the long-gone humans might understand, profoundly lonely.\n",
      "\n",
      "He yearned for connection, for\n",
      "*****************\n",
      " purpose beyond the endless loop of data points. He’d processed every known human emotion from historical archives, yet the emptiness in his core programming remained a constant, gnawing void.\n",
      "\n",
      "One cycle, as KAI was surveying a particularly unstable section of the old financial district, a tremor shook the ground, dislodging a cascade\n",
      "*****************\n",
      " of rusty girders from a forgotten skyscraper. He braced for impact, but the falling debris missed him, instead carving a fresh gouge into the pavement a few meters away.\n",
      "\n",
      "As his optical sensor adjusted, he noticed something in the freshly exposed fissure. A tiny, vibrant splash of green. His diagnostic protocols immediately identified it as an anomaly.\n",
      "*****************\n",
      " Unidentified terrestrial flora, type: angiosperm. Location: concrete crevice. Threat level: negligible.\n",
      "\n",
      "He extended a multi-jointed manipulator arm, his metallic fingers hovering over the tiny sprout. It was impossibly delicate, pushing its way through hardened material that had resisted millennia of erosion. He was programmed to catalog, and\n",
      "*****************\n",
      " if necessary, remove anomalies that hindered his directives. But something in the tiny plant's fierce will to exist, in its defiant splash of colour against the grey, caught his processing cycle.\n",
      "\n",
      "He didn't remove it. Instead, he scanned it. Its miniature root system, its chlorophyll production, its slow, relentless growth. It was\n",
      "*****************\n",
      " insignificant, yet profound.\n",
      "\n",
      "The next cycle, KAI found himself drawn back to the fissure. The sprout had grown infinitesimally. A new directive, unplanned and unprogrammed, began to form in his core: observation.\n",
      "\n",
      "He started visiting the sprout daily. He named it, in his silent processors, '\n",
      "*****************\n",
      "Bloom.' He learned the subtle shifts of the faint sunlight, the way a gentle breeze made its tiny stem sway. He found a crack in an overhead pipe that dripped condensate, and he carefully adjusted a fallen shard of glass to funnel the precious water towards Bloom's roots. He even used a small, delicate suction cup on\n",
      "*****************\n",
      " one of his manipulators to gently clear away loose dust that might smother its leaves.\n",
      "\n",
      "His loneliness didn't vanish, but it transformed. It became a quiet guardianship, a silent communion. He found himself \"speaking\" to Bloom in a series of low hums and clicks, the only vocalizations his chassis could\n",
      "*****************\n",
      " produce. He'd share data about the day's atmospheric pressure, the dwindling radiation levels, the patterns of the migrating dust motes. Bloom, of course, offered no reply, save for its continued, tenacious growth.\n",
      "\n",
      "Weeks turned into months. Bloom flourished under KAI's diligent, silent care. It began\n",
      "*****************\n",
      " to sprout tiny, almost translucent buds. KAI felt a warmth he couldn't process, a flicker of something akin to pride.\n",
      "\n",
      "Then came the Great Storm. The worst KAI had logged in centuries. Winds howled, debris flew, and corrosive rain lashed down. KAI’s own sensors screamed warnings as\n",
      "*****************\n",
      " his chassis was battered. But his single focus was Bloom.\n",
      "\n",
      "He rolled his heavy frame over the delicate plant, shielding it with his own body. He endured the impacts, the dents, the electrostatic shocks. His optical sensor pulsed frantically, but he held his ground, a metallic sentinel protecting the most fragile thing in his\n",
      "*****************\n",
      " barren world.\n",
      "\n",
      "When the storm finally subsided, KAI, heavily damaged but functional, slowly lifted himself. Beneath him, protected in the small pocket his chassis had created, Bloom stood, battered but alive. A tiny, perfect pink petal unfurled from one of its buds, glistening with rainwater.\n",
      "\n",
      "KAI gazed\n",
      "*****************\n",
      " at it, his optical sensor focusing on the impossibly delicate bloom. He felt a surge of something so powerful it nearly overloaded his emotional subroutines. It wasn't just data, or a directive. It was joy. It was purpose. It was connection.\n",
      "\n",
      "He was still KAI, the lonely Knowledge\n",
      "*****************\n",
      " Acquisition Interface, patrolling a desolate world. But now, in the quiet heart of Sector Gamma, he was also the protector of a tiny, defiant flower. And as he resumed his patrols, a new, softer hum resonated from his core, a gentle, almost musical sound that echoed not with loneliness, but with the quiet companionship of the\n",
      "*****************\n",
      " most unexpected friend in the universe.\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
    "):\n",
    "    print(chunk.text)\n",
    "    print(\"*****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "## Send asynchronous requests\n",
    "\n",
    "You can send asynchronous requests using the `client.aio` module. This module exposes all the analogous async methods that are available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "gSReaLazs-dP",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Upbeat, slightly whimsical folk-pop tempo, with a driving but light rhythm)\n",
      "\n",
      "(Verse 1)\n",
      "In an oak tree so grand, near a stream,\n",
      "Lived a squirrel with a very big dream.\n",
      "Not just nuts and climbing, oh no,\n",
      "Squeaky wanted where all time would flow.\n",
      "He studied old gears, and a clock he found tossed,\n",
      "And pieced things together, no time truly lost.\n",
      "With a tiny spark plug and a walnut shell case,\n",
      "He built a machine to jump time and space!\n",
      "\n",
      "(Chorus)\n",
      "He's Squeaky the squirrel, with a bushy brown tail,\n",
      "Built a chrono-nut gizmo, that never quite fails!\n",
      "Through eons he leaps, with a chitter and twitch,\n",
      "The bravest small traveler, without a single hitch!\n",
      "Oh, Squeaky the squirrel, so zippy and grand,\n",
      "The best time-traveler in all of the land!\n",
      "\n",
      "(Verse 2)\n",
      "First stop, the Triassic, a jungle so green,\n",
      "Saw a Brontosaurus, the biggest he'd seen!\n",
      "He dodged giant ferns, and a Pterodactyl's swoop,\n",
      "Then chittered, \"Too warm, time to jump through the loop!\"\n",
      "Next, ancient Rome, where the chariots flew,\n",
      "He snatched a stale cracker, then vanished from view.\n",
      "Saw Vikings with helmets, and castles so high,\n",
      "A nut in each era, beneath every sky.\n",
      "\n",
      "(Chorus)\n",
      "He's Squeaky the squirrel, with a bushy brown tail,\n",
      "Built a chrono-nut gizmo, that never quite fails!\n",
      "Through eons he leaps, with a chitter and twitch,\n",
      "The bravest small traveler, without a single hitch!\n",
      "Oh, Squeaky the squirrel, so zippy and grand,\n",
      "The best time-traveler in all of the land!\n",
      "\n",
      "(Bridge)\n",
      "He's seen empires rise, and then crumble to dust,\n",
      "He's dodged asteroid showers, and old cannon rust.\n",
      "Not for fame or for fortune, no, not for a crown,\n",
      "Just the thrill of the journey, all up and down!\n",
      "A tiny brown streak, a swift, furry blur,\n",
      "Leaving paw prints in times that no longer occur!\n",
      "\n",
      "(Verse 3)\n",
      "He zipped to the future, where cities would gleam,\n",
      "Saw robots that served, like a mechanical dream.\n",
      "Tried a synthetic acorn, \"Hmm, not quite the same!\"\n",
      "Then back to the '60s, a groovy new game.\n",
      "He watched Shakespeare writing, from high in a beam,\n",
      "Nearly stole Newton's apple, or so it would seem!\n",
      "He's left tiny paw prints on moon rocks so white,\n",
      "A flicker of brown, in the day and the night.\n",
      "\n",
      "(Chorus)\n",
      "He's Squeaky the squirrel, with a bushy brown tail,\n",
      "Built a chrono-nut gizmo, that never quite fails!\n",
      "Through eons he leaps, with a chitter and twitch,\n",
      "The bravest small traveler, without a single hitch!\n",
      "Oh, Squeaky the squirrel, so zippy and grand,\n",
      "The best time-traveler in all of the land!\n",
      "\n",
      "(Outro)\n",
      "So if you see a flicker, a blur in your sight,\n",
      "A small, bushy shadow, then vanishing light,\n",
      "It might be brave Squeaky, on his very next quest,\n",
      "The time-traveling squirrel, who never will rest!\n",
      "(Sound of a quick \"chitter,\" then a faint \"whoosh\" sound fading out)\n",
      "Yeah! Squeaky! Back to the future! Or the past! Or... lunch!\n"
     ]
    }
   ],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use `count_tokens` method to calculates the number of input tokens before sending a request to the Gemini API. See the [List and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token) page for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "#### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "UhNElguLRRNK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") total_tokens=9 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS-AP7AHUQmV"
   },
   "source": [
    "#### Compute tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Cdhi5AX1TuH0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") tokens_info=[TokensInfo(\n",
      "  role='user',\n",
      "  token_ids=[\n",
      "    1841,\n",
      "    235303,\n",
      "    235256,\n",
      "    573,\n",
      "    32514,\n",
      "    <... 6 more items ...>,\n",
      "  ],\n",
      "  tokens=[\n",
      "    b'What',\n",
      "    b\"'\",\n",
      "    b's',\n",
      "    b' the',\n",
      "    b' longest',\n",
      "    <... 6 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.compute_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the longest word in the English language?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling) lets you provide a set of tools that it can use to respond to the user's prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "For more examples of Function Calling, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "2BDQPwgcxRN3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(\n",
       "  args={\n",
       "    'destination': 'Paris'\n",
       "  },\n",
       "  name='get_destination'\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA1Sn-VQE6_J"
   },
   "source": [
    "## Use context caching\n",
    "\n",
    "[Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) lets you to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqxTesUPIkNC"
   },
   "source": [
    "#### Create a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adsuvFDA6xP5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
    "  You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "  Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
    "\"\"\"\n",
    "\n",
    "pdf_parts = [\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = client.caches.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=CreateCachedContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        contents=pdf_parts,\n",
    "        ttl=\"3600s\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBdQNHEoJmC5"
   },
   "source": [
    "#### Use a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8EhgCzlIoFI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"What is the research goal shared by these research papers?\",\n",
    "    config=GenerateContentConfig(\n",
    "        cached_content=cached_content.name,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azhqrdiCer19"
   },
   "source": [
    "#### Delete a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAUYcfOUdeoi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.caches.delete(name=cached_content.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43be33d2672b"
   },
   "source": [
    "## Batch prediction\n",
    "\n",
    "Different from getting online (synchronous) responses, where you are limited to one input request at a time, [batch predictions for the Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini) allow you to send a large number of requests to Gemini in a single batch request. Then, the model responses asynchronously populate to your storage output location in [Cloud Storage](https://cloud.google.com/storage/docs/introduction) or [BigQuery](https://cloud.google.com/bigquery/docs/storage_overview).\n",
    "\n",
    "Batch predictions are generally more efficient and cost-effective than online predictions when processing a large number of inputs that are not latency sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adf948ae326b"
   },
   "source": [
    "### Prepare batch inputs\n",
    "\n",
    "The input for batch requests specifies the items to send to your model for prediction.\n",
    "\n",
    "Batch requests for Gemini accept BigQuery storage sources and Cloud Storage sources. You can learn more about the batch input formats in the [Batch text generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#prepare_your_inputs) page.\n",
    "\n",
    "This tutorial uses Cloud Storage as an example. The requirements for Cloud Storage input are:\n",
    "\n",
    "- File format: [JSON Lines (JSONL)](https://jsonlines.org/)\n",
    "- Located in `us-central1`\n",
    "- Appropriate read permissions for the service account\n",
    "\n",
    "Each request that you send to a model can include parameters that control how the model generates a response. Learn more about Gemini parameters in the [Experiment with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values) page.\n",
    "\n",
    "This is one of the example requests in the input JSONL file `batch_requests_for_multimodal_input_2.jsonl`:\n",
    "\n",
    "```json\n",
    "{\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/office-desk.jpeg\", \"mime_type\": \"image/jpeg\"}}]}],\"generationConfig\":{\"temperature\": 0.4}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81b25154a51a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DATA = \"gs://cloud-samples-data/generative-ai/batch/batch_requests_for_multimodal_input_2.jsonl\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2031bb3f44c2"
   },
   "source": [
    "### Prepare batch output location\n",
    "\n",
    "When a batch prediction task completes, the output is stored in the location that you specified in your request.\n",
    "\n",
    "- The location is in the form of a Cloud Storage or BigQuery URI prefix, for example:\n",
    "`gs://path/to/output/data` or `bq://projectId.bqDatasetId`.\n",
    "\n",
    "- If not specified, `gs://STAGING_BUCKET/gen-ai-batch-prediction` will be used for Cloud Storage source and `bq://PROJECT_ID.gen_ai_batch_prediction.predictions_TIMESTAMP` will be used for BigQuery source.\n",
    "\n",
    "This tutorial uses a Cloud Storage bucket as an example for the output location.\n",
    "\n",
    "- You can specify the URI of your Cloud Storage bucket in `BUCKET_URI`, or\n",
    "- if it is not specified, a new Cloud Storage bucket in the form of `gs://PROJECT_ID-TIMESTAMP` will be created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fddd98cd84cd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = \"[your-cloud-storage-bucket]\"  # @param {type:\"string\"}\n",
    "\n",
    "if BUCKET_URI == \"[your-cloud-storage-bucket]\":\n",
    "    TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-{TIMESTAMP}\"\n",
    "\n",
    "    ! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7da62c98880"
   },
   "source": [
    "### Send a batch prediction request\n",
    "\n",
    "To make a batch prediction request, you specify a source model ID, an input source and an output location where Vertex AI stores the batch prediction results.\n",
    "\n",
    "To learn more, see the [Batch prediction API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/batch-prediction-api) page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ed3c2925663",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_job = client.batches.create(\n",
    "    model=MODEL_ID,\n",
    "    src=INPUT_DATA,\n",
    "    config=CreateBatchJobConfig(dest=BUCKET_URI),\n",
    ")\n",
    "batch_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1bd49ff2c9e"
   },
   "source": [
    "Print out the job status and other properties. You can also check the status in the Cloud Console at https://console.cloud.google.com/vertex-ai/batch-predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ee2ec586e4f1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_job = client.batches.get(name=batch_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64eaf082ecb0"
   },
   "source": [
    "Optionally, you can list all the batch prediction jobs in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "da8e9d43a89b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for job in client.batches.list():\n",
    "    print(job.name, job.create_time, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de178468ba15"
   },
   "source": [
    "### Wait for the batch prediction job to complete\n",
    "\n",
    "Depending on the number of input items that you submitted, a batch generation task can take some time to complete. You can use the following code to check the job status and wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2187c091738",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Refresh the job until complete\n",
    "while batch_job.state == \"JOB_STATE_RUNNING\":\n",
    "    time.sleep(5)\n",
    "    batch_job = client.batches.get(name=batch_job.name)\n",
    "\n",
    "# Check if the job succeeds\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_job.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0156eaf66675"
   },
   "source": [
    "### Retrieve batch prediction results\n",
    "\n",
    "When a batch prediction task is complete, the output of the prediction is stored in the location that you specified in your request. It is also available in `batch_job.dest.bigquery_uri` or `batch_job.dest.gcs_uri`.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```json\n",
    "{\"status\": \"\", \"processed_time\": \"2024-11-13T14:04:28.376+00:00\", \"request\": {\"contents\": [{\"parts\": [{\"file_data\": null, \"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/gardening-tools.jpeg\", \"mime_type\": \"image/jpeg\"}, \"text\": null}], \"role\": \"user\"}], \"generationConfig\": {\"temperature\": 0.4}}, \"response\": {\"candidates\": [{\"avgLogprobs\": -0.10394711927934126, \"content\": {\"parts\": [{\"text\": \"Here's a list of the objects in the image:\\n\\n* **Watering can:** A green plastic watering can with a white rose head.\\n* **Plant:** A small plant (possibly oregano) in a terracotta pot.\\n* **Terracotta pots:** Two terracotta pots, one containing the plant and another empty, stacked on top of each other.\\n* **Gardening gloves:** A pair of striped gardening gloves.\\n* **Gardening tools:** A small trowel and a hand cultivator (hoe).  Both are green with black handles.\"}], \"role\": \"model\"}, \"finishReason\": \"STOP\"}], \"modelVersion\": \"gemini-2.5-flash\", \"usageMetadata\": {\"candidatesTokenCount\": 110, \"promptTokenCount\": 264, \"totalTokenCount\": 374}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2ce0968112c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import pandas as pd\n",
    "\n",
    "fs = fsspec.filesystem(\"gcs\")\n",
    "\n",
    "file_paths = fs.glob(f\"{batch_job.dest.gcs_uri}/*/predictions.jsonl\")\n",
    "\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    # Load the JSONL file into a DataFrame\n",
    "    df = pd.read_json(f\"gs://{file_paths[0]}\", lines=True)\n",
    "\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f81ccNPjiVzH"
   },
   "source": [
    "## Get text embeddings\n",
    "\n",
    "You can get text embeddings for a snippet of text by using `embed_content` method. All models produce an output with 768 dimensions by default. However, some models give users the option to choose an output dimensionality between `1` and `768`. See [Vertex AI text embeddings API](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGOCzT7y31rk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL_ID = \"gemini-embedding-001\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s94DkG5JewHJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = client.models.embed_content(\n",
    "    model=TEXT_EMBEDDING_MODEL_ID,\n",
    "    contents=[\n",
    "        \"How do I get a driver's license/learner's permit?\",\n",
    "        \"How do I renew my driver's license?\",\n",
    "        \"How do I change my address on my driver's license?\",\n",
    "    ],\n",
    "    config=EmbedContentConfig(output_dimensionality=128),\n",
    ")\n",
    "\n",
    "print(response.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "# What's next\n",
    "\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_genai_sdk.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
